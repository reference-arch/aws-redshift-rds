#Redshift To RDS Without RDS Table create Sample 

This sample demonstrates how you can use Data Pipeline to copy data from a Redshift table to a RDS table. The template assumes that the table is already created in RDS. To make this sample to work, you must ensure you have the following:

* myRDSDatabaseName: Rds Database Instance Name (DB Instance)
* myRedshiftInstanceId: Redshift Cluster name (Cluster)
* myRDSInstanceId: RDS database name
* myRedshiftDatabaseName: Redshift database name
* myRedshiftUsername/myRDSUsername: Redshift username and password.  This user must have write access to the table where data will be copied to.
* myRDSUsername/myRDSPassword: RDS username and password.  This user must have write access to the table where data will be copied to.
* myRdsTableName: RDS table name to be used.
* myRedshiftTableName: Redshift table name to be used.
* myEc2SecurityGrps: Ec2 Security group so that the resource can access both Redshift and RDS instance.
* myInsertQueryPreparedStatement: RDS Insert prepared statement to be used.
* myOutputS3Path: Temporary S3 location to be used for copying.
* myS3LogsPath: S3 location to direct log messages generated by Data Pipeline.  

You will need to provide the above information in the "put-pipeline-definition" command below.

##Running this sample

```sh
 $> aws datapipeline create-pipeline --name redshift_to_rds_without_table_create --unique-id redshift_to_rds_without_table_create 

# You receive a pipeline activity like this. 
#   -----------------------------------------
#   |             CreatePipeline             |
#   +-------------+--------------------------+
#   |  pipelineId | df-032542760T7M3UXU8KJ   |
#   +-------------+--------------------------+

#now upload the pipeline definition 

 $> aws datapipeline put-pipeline-definition --pipeline-id df-032542760T7M3UXU8KJ --pipeline-definition \
    file://RedshiftToRDS_WithoutRDSCreate.json --parameter-values myRDSDatabaseName=<rds_database_name>  \
    *myRDSPassword=<rds_password> myRDSInstanceId=<rds_instance> myRDSUsername=<rds_username> \
    myRedshiftDatabaseName=<redshift_database_name> *myRedshiftPassword=<redshift_password> \
    myRedshiftInstanceId=<redshift_instance> myRedshiftUsername=<redshift_username> myRedshiftTableName=<redshift_tablename> \
    myRDSTableName=<rds_tablename> myInsertQueryPreparedStatement="<insert prepared statement>" \ 
    myEc2SecurityGrps=<ec2-security-group-name> myS3LogsPath<log-path> myOutputS3Path=<temporary-log-path> 

# You receive a validation messages like this

#   ----------------------- 
#   |PutPipelineDefinition|
#   +-----------+---------+
#   |  errored  |  False  |
#   +-----------+---------+

#now activate the pipeline
  $> aws datapipeline activate-pipeline --pipeline-id df-032542760T7M3UXU8KJ


#check the status of your pipeline 

  $> aws datapipeline list-runs --pipeline-id df-032542760T7M3UXU8KJ
#       Name                                                Scheduled Start      Status                 
#       ID                                                  Started              Ended              
#---------------------------------------------------------------------------------------------------
#   1.  ActivityId_vmVn4                                    2015-11-06T23:52:04  WAITING_FOR_RUNNER     
#       @ActivityId_vmVn4_2015-11-06T23:52:04               2015-11-06T23:52:11                     
#
#   2.  ResourceId_idL0Y                                    2015-11-06T23:52:04  CREATING               
#       @ResourceId_idL0Y_2015-11-06T23:52:04               2015-11-06T23:52:11      
```

#Redshift To RDS with RDS Table create Sample

This sample demonstrates how you can use Data Pipeline to copy data from a Redshift table to a RDS table. The template assumes that the table is already created in RDS. To make this sample to work, you must ensure you have the following:

* myRDSDatabaseName: Rds Database Instance Name (DB Instance)
* myRedshiftInstanceId: Redshift Cluster name (Cluster)
* myRDSInstanceId: RDS database name
* myRedshiftDatabaseName: Redshift database name
* myRedshiftUsername/myRDSUsername: Redshift username and password.  This user must have write access to the table where data will be copied to.
* myRDSUsername/myRDSPassword: RDS username and password.  This user must have write access to the table where data will be copied to.
* myRdsTableName: RDS table name to be used.
* myRedshiftTableName: Redshift table name to be used.
* myEc2SecurityGrps: Ec2 Security group so that the resource can access both Redshift and RDS instance.
* myInsertQueryPreparedStatement: RDS Insert prepared statement to be used.
* myCreateTableQuery: RDS create table query statement.
* myOutputS3Path: Temporary S3 location to be used for copying.
* myS3LogsPath: S3 location to direct log messages generated by Data Pipeline.

You will need to provide the above information in the "put-pipeline-definition" command below.

##Running this sample

```sh
 $> aws datapipeline create-pipeline --name redshift_to_rds_with_table_create --unique-id redshift_to_rds_with_table_create

# You receive a pipeline activity like this.
#   -----------------------------------------
#   |             CreatePipeline             |
#   +-------------+--------------------------+
#   |  pipelineId | df-032542760T7M3UXU8KJ   |
#   +-------------+--------------------------+

#now upload the pipeline definition

 $> aws datapipeline put-pipeline-definition --pipeline-id df-032542760T7M3UXU8KJ --pipeline-definition \
    file://RedshiftToRDS_withTableCreate.json --parameter-values myRDSDatabaseName=<rds_database_name>  \
    *myRDSPassword=<rds_password> myRDSInstanceId=<rds_instance> myRDSUsername=<rds_username> \
    myRedshiftDatabaseName=<redshift_database_name> *myRedshiftPassword=<redshift_password> \
    myRedshiftInstanceId=<redshift_instance> myRedshiftUsername=<redshift_username> myRedshiftTableName=<redshift_tablename> \
    myRDSTableName=<rds_tablename> myInsertQueryPreparedStatement="<insert prepared statement>" \
    myEc2SecurityGrps=<ec2-security-group-name> myS3LogsPath<log-path> myCreateTableQuery=<rds_create_table> myOutputS3Path=<temporary-log-path>

# You receive a validation messages like this

#   -----------------------
#   |PutPipelineDefinition|
#   +-----------+---------+
#   |  errored  |  False  |
#   +-----------+---------+

#now activate the pipeline
  $> aws datapipeline activate-pipeline --pipeline-id df-032542760T7M3UXU8KJ


#check the status of your pipeline

  $> aws datapipeline list-runs --pipeline-id df-032542760T7M3UXU8KJ
#       Name                                                Scheduled Start      Status
#       ID                                                  Started              Ended
#---------------------------------------------------------------------------------------------------
#   1.  ActivityId_vmVn4                                    2015-11-06T23:52:04  WAITING_FOR_RUNNER
#       @ActivityId_vmVn4_2015-11-06T23:52:04               2015-11-06T23:52:11
#
#   2.  ResourceId_idL0Y                                    2015-11-06T23:52:04  CREATING


##Related documentation
https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html
https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html

